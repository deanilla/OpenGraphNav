# Pipline

1.  **指令解析与任务分解**：先用 LLM 将instructions分解为一系列更小、更明确的**子任务**（Subtask）。每个子任务可能包含动作（如“走”、“停”）、方向（如“左”、“右”）、空间关系（如“在...旁边”、“在...前面”）和地标（如“窗户”、“椅子”）。这部分参考了CA-Nav，为后续基于子任务队列的进度管理奠基。

2.  **结构化环境感知与建图**：agent在环境中移动时，不再仅仅依靠文本描述来理解环境而是，构建并维护一个**增量式场景图**（Incremental Scene Graph）。这部分来自SG-Nav和UniGoal的场景图的设计。
    *   这个图是一个结构化的知识库，以**节点**（代表航点、物体、物体组、房间）和**边**（代表它们之间的空间或语义关系，如“在左边”、“属于”）来表示环境。
    *   每当智能体移动到一个新的位置（航点），它会观察周围环境。
    *   通过调用专门的视觉模型（如 Recognize Anything Model 用于物体识别，SpatialBot 用于理解空间关系），智能体将当前观察到的物体和它们之间的关系添加到场景图中。

3.  **基于结构化信息的时空推理**：这是原来 Open-Nav 的决策核心 Spatial-Temporal Chain-of-Thought, CoT。几乎没有改动。
    *   **获取当前目标**：从任务分解得到的子任务队列中取出当前需要完成的子任务。
    *   **提取局部环境上下文**：从维护的场景图中，提取以**当前所在航点**为中心的**局部子图**。这个子图包含了附近的关键物体和它们之间的关系。这参考了SG-Nav的决策，但是从Object-goal扩展到了VLN。
    *   **LLM 决策**：将当前子任务、局部场景图的结构化描述以及候选的可移动航点信息一并作为上下文，输入给 LLM。
    *   **生成决策**：LLM 基于这些结构化的、精确的时空信息进行推理，判断哪个候选航点最有助于完成当前的子任务，并输出其选择。

4.  **进度估计与任务队列更新**：在执行动作后，智能体需要判断当前的子任务是否已经完成，从而实现不是基于纯文本的进度控制。
    *   Open-Nav 会再次调用 LLM，将当前的子任务和在新航点观察到的环境信息（可能也来自场景图）作为输入。
    *   LLM 评估这个子任务是否达成。如果达成，该子任务就会从队列中移除（**出队**），智能体开始执行下一个子任务。

5.  **循环直至完成**：这个过程（感知 -> 更新场景图 -> 推理 -> 决策 -> 执行 -> 评估进度）不断循环，智能体一步步完成所有子任务，最终到达指令指定的目标位置。
